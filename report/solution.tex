\section{Solution}

\subsection{Classification}
In this competition, since the data is organized in a time-series format, it is
intuitive to represent the data using the Hidden Markov Model~\cite{HMM1996}
which is a generative probabilistic model, in which a sequence of observable
variables (which in our case, is the X, Y, Z coordinate) is generated by a
sequence of internal hidden state.

In this dataset, we postulate that each hidden state should correspond to the
activity that the users are currently carrying out such as running, walking and
so on. However, it is hard for us to model what will be the possible different
activities of each user. Having too much states will cause an overfit for our
training set while too few states will cause an underfit. To that end, we
perform a limited exhaustive enumeration of the hidden states to compute the
optimal performance.

Additionally, for each of the mobile users, there will be certain phone
position in which the user will have the phone under. At some time, the users
might shake the phones slightly while at rarer times, the users might move the
phones much faster. Thus, we model our observable variables as Gaussian
variables.

Our next insight is that although for each user there will be a certain
coordinate is "popular" with a particular user, there will be other coordinates
that will also be "popular" for the user. To account for such possibilities, we
model each of our coordinate as a Gaussian mixtures~\cite{GMM2009}. Again, it
is impossible to evaluate what will be the optimal number of Gaussian mixtures
to model a coordinate which we then resort (again) to a limited exhaustive
enumeration.

Finally, due to the time-series nature of the data, we postulate that we could
perform k-fold cross-validation~\cite{KFold2013} in which the data set is
divided into k consecutive subsets (i.e. no shuffling), and then we perform
cross-validation method k times. Each time, one of the k subsets is being left
out and the other k-1 subsets are put together to form a training set. For our
purpose, we build k HMM model and each of the questions is then being evaluated
against these models.

Incorporating the above insights to our model has yielded a \textbf{79.4\%}
classification accuracy on the Kaggle leaderboard.

\subsection{Clustering}
In the previous section, we knew which device id we needed the test sequence
to compare to. If this information was not available to us, we would need
to do a linear scan through every device in the training set for every
test sequence. Hence, the runtime is \textit{O}($nm$) where $n$ is the
number of device ids and $m$ the number of test sequences. With our
training and test data files, it took us ~15 hours to do classification.

An interesting sub-problem would be to find a cheap and efficient way of
doing search space reduction. We decided to apply the k-means clustering
algorithm~\cite{kmeans} to create $\sqrt{387} \simeq 20$ clusters, and only build HMMs
after finding which cluster the sequence is most likely part of. We used both
euclidian distance and cosine similarity as our distance measures between
clusters.

\begin{program}
\mbox{\textit{\textbf{k-means(devices, k)}}}
\BEGIN \\
  samples := randomsample(devices, k) \\
  clusters := [] \\
  \FOR sample <- samples \DO
    clusters.add([sample]) \OD
  means := findmeans(clusters)
  \WHILE is\ not\ converged(clusters) \DO
    clusters := [[]\ for\ k]
    \FOR device <- devices \DO
      \FOR i <- 0..k \DO
        m = means[i]
        d = distance(device, m)
        \IF is\ best\ distance(d)
          clusters[i].add(device)
        \OD
        \OD
    means = findmeans(clusters)
    \OD
\END
\end{program}

The distance() function can be implemented with euclidian distance. In this
situation, we observe that the sizes of our clusters are very skewed.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{./figs/euclidian.png}
  \label{fig:euclidian}
\end{figure}

However, if we use cosine similarity as a distance measurements we get even
cluster sizes. Therefore, it is preferable to use cosine similarity as
the distance measurement.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{./figs/cosine.png}
  \label{fig:cosine}
\end{figure}
